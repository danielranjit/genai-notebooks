{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Y0r19Litu1yTVwvcGAGgt1IF"
      },
      "source": [
        "!pip install --upgrade --user google-cloud-aiplatform>=1.29.0 google-cloud-storage langchain pypdf ratelimit backoff langchain-google-vertexai google-cloud-bigquery"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Y0r19Litu1yTVwvcGAGgt1IF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "HMBswhDCuk2E"
      },
      "execution_count": null,
      "outputs": [],
      "id": "HMBswhDCuk2E"
    },
    {
      "cell_type": "code",
      "source": [
        "# get project ID\n",
        "PROJECT_ID = ! gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "DOCUMENT_URL = \"https://www.nyc.gov/assets/doh/downloads/pdf/rii/fpc-manual.pdf\" # @param {type:\"string\"}\n",
        "\n",
        "# define project information manually if the above code didn't work\n",
        "if PROJECT_ID == \"(unset)\":\n",
        "  PROJECT_ID = \"[your-project-id]\" # @param {type:\"string\"}\n",
        "\n",
        "print(PROJECT_ID)"
      ],
      "metadata": {
        "id": "it17qlHBurGV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "it17qlHBurGV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Vertex AI"
      ],
      "metadata": {
        "id": "MfUnQ4QIc2aT"
      },
      "id": "MfUnQ4QIc2aT"
    },
    {
      "cell_type": "code",
      "source": [
        "# init the aiplatform package\n",
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "47xjFhdUhYYY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "47xjFhdUhYYY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just test the Embeddings model"
      ],
      "metadata": {
        "id": "1jujZHnvc60j"
      },
      "id": "1jujZHnvc60j"
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "def text_embedding(text_to_embed) -> list:\n",
        "    \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
        "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@002\")\n",
        "    embeddings = model.get_embeddings([text_to_embed])\n",
        "    for embedding in embeddings:\n",
        "        vector = embedding.values\n",
        "    return vector"
      ],
      "metadata": {
        "id": "vtO0EPFDvz81"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vtO0EPFDvz81"
    },
    {
      "cell_type": "code",
      "source": [
        "emb1 = text_embedding(\"Hello World\")\n",
        "print(emb1)"
      ],
      "metadata": {
        "id": "aTkaz6IixJbY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aTkaz6IixJbY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the PDF and Split it into pages"
      ],
      "metadata": {
        "id": "LdRF_G0pgh1I"
      },
      "id": "LdRF_G0pgh1I"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf = PyPDFLoader(DOCUMENT_URL)\n",
        "pages = pdf.load_and_split()\n",
        "\n",
        "# convert pages array into an array of page_content\n",
        "pages = [page.page_content for page in pages]"
      ],
      "metadata": {
        "id": "809rZH6afr8Z"
      },
      "execution_count": null,
      "outputs": [],
      "id": "809rZH6afr8Z"
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pages))\n",
        "print(pages[125][:200])\n",
        "print(len(pages[0]))"
      ],
      "metadata": {
        "id": "ic5E9Ba-i5AE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ic5E9Ba-i5AE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an Embedding from 1 page"
      ],
      "metadata": {
        "id": "d5-BEt5Fig_n"
      },
      "id": "d5-BEt5Fig_n"
    },
    {
      "cell_type": "code",
      "source": [
        "emb1 = text_embedding(pages[0])\n",
        "print(emb1)"
      ],
      "metadata": {
        "id": "rcKzyLhVigF_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rcKzyLhVigF_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the model to clean up all the pages"
      ],
      "metadata": {
        "id": "95nxQp7qMnZ3"
      },
      "id": "95nxQp7qMnZ3"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the prompt we will use to clean up all the pages\n",
        "cleanup_prompt = \"\"\"\n",
        "context: Edit the following data surrounded by triple back ticks.\n",
        "\n",
        "1. Correct spelling and grammar mistakes.\n",
        "2. Remove data not related to restaurant and food safety.\n",
        "3. Return the edited data.\n",
        "\n",
        "```\n",
        "Data: {0}\n",
        "```\n",
        "cleaned data:\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Yv-qlT3v5AP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2Yv-qlT3v5AP"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.aiplatform_v1beta1.types.content import SafetySetting, HarmCategory\n",
        "\n",
        "cleaned_pages = []\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "def generate(prompt):\n",
        "  model = GenerativeModel(\"gemini-pro\")\n",
        "  response = model.generate_content(\n",
        "    prompt,\n",
        "    generation_config={\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 0.5,\n",
        "        \"top_p\": 0.5,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "  stream=False,\n",
        "  safety_settings=safety_settings,\n",
        "  )\n",
        "  try:\n",
        "    return response.text\n",
        "  except:\n",
        "    print(\"An Error Ocuured Creaning the Data\")\n",
        "    return \"An Error Ocuured Creaning the Data\"\n",
        "\n",
        "# Iterate over the pages and generate a summary for each page\n",
        "for page in pages:\n",
        "    # Create a prompt for the model using the page and a prompt template\n",
        "    prompt = cleanup_prompt.format(page)\n",
        "\n",
        "    # Generate a summary using the model and the prompt\n",
        "    cleaned_page = generate(prompt=prompt)\n",
        "\n",
        "    # Append the summary to the list of summaries\n",
        "    cleaned_pages.append(cleaned_page)\n",
        "\n",
        "    #print the number of times through the loop\n",
        "    print(len(cleaned_pages))"
      ],
      "metadata": {
        "id": "O_s-QzNOfrb8"
      },
      "id": "O_s-QzNOfrb8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate the embeddings in batches\n",
        "\n",
        "Below are helper functions that are used to rate limit and batch the geberation of the embeddings."
      ],
      "metadata": {
        "id": "Okd3WPTndN3B"
      },
      "id": "Okd3WPTndN3B"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Generator, List, Optional, Tuple\n",
        "import functools\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "\n",
        "# Define an embedding method that uses the model\n",
        "def encode_texts_to_embeddings(chunks: List[str]) -> List[Optional[List[float]]]:\n",
        "    try:\n",
        "        model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@002\")\n",
        "\n",
        "        # convert chunks into list[TextEmbeddingInput]\n",
        "        inputs = [TextEmbeddingInput(text=chunk, task_type=\"RETRIEVAL_DOCUMENT\") for chunk in chunks]\n",
        "        embeddings = model.get_embeddings(inputs)\n",
        "\n",
        "        # You could also generate the embeddings without the task_type.\n",
        "        # Then, you are just passing a collection of strings. In a real app\n",
        "        # test it multiple ways.\n",
        "        # The alternative would be as follows\n",
        "        # embeddings = model.get_embeddings(chunks)\n",
        "\n",
        "        return [embedding.values for embedding in embeddings]\n",
        "    except Exception:\n",
        "        return [None for _ in range(len(chunks))]\n",
        "\n",
        "\n",
        "# Generator function to yield batches of descriptions\n",
        "def generate_batches(\n",
        "    chunks: List[str], batch_size: int\n",
        ") -> Generator[List[str], None, None]:\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        yield chunks[i : i + batch_size]\n",
        "\n",
        "\n",
        "def encode_text_to_embedding_batched(\n",
        "    chunks: List[str], api_calls_per_minute: int = 20, batch_size: int = 5\n",
        ") -> Tuple[List[bool], np.ndarray]:\n",
        "\n",
        "    embeddings_list: List[List[float]] = []\n",
        "\n",
        "    # Prepare the batches using a generator\n",
        "    batches = generate_batches(chunks, batch_size)\n",
        "\n",
        "    seconds_per_job = 60 / api_calls_per_minute\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for batch in tqdm(\n",
        "            batches, total=math.ceil(len(chunks) / batch_size), position=0\n",
        "        ):\n",
        "            futures.append(\n",
        "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
        "            )\n",
        "            time.sleep(seconds_per_job)\n",
        "\n",
        "        for future in futures:\n",
        "            embeddings_list.extend(future.result())\n",
        "\n",
        "    is_successful = [\n",
        "        embedding is not None for sentence, embedding in zip(chunks, embeddings_list)\n",
        "    ]\n",
        "    embeddings_list_successful = np.squeeze(\n",
        "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
        "    )\n",
        "    return is_successful, embeddings_list_successful\n"
      ],
      "metadata": {
        "id": "m2D8TdYb3jgd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "m2D8TdYb3jgd"
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = encode_text_to_embedding_batched(cleaned_pages, api_calls_per_minute=100)"
      ],
      "metadata": {
        "id": "-Hitlbcu4mpw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-Hitlbcu4mpw"
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_array = embeddings[1]\n",
        "ids = [i for i in range(len(pages))]"
      ],
      "metadata": {
        "id": "lhlyl_y9lLBJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lhlyl_y9lLBJ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(cleaned_pages))\n",
        "print(len(pages))\n",
        "print(len(ids))\n",
        "print(len(embeddings_array))\n",
        "print(\"--------------------------\")\n",
        "print(ids[50])\n",
        "print(pages[50][:50])\n",
        "print(cleaned_pages[50][:50])\n",
        "print(embeddings_array[50][:5])"
      ],
      "metadata": {
        "id": "NFE7HnDKgdkg"
      },
      "id": "NFE7HnDKgdkg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the JSON file to Import into BigQuery\n",
        "\n"
      ],
      "metadata": {
        "id": "ajyQsX_GpTEM"
      },
      "id": "ajyQsX_GpTEM"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "FILE_NAME = \"embeddings_for_bq.jsonl\"\n",
        "\n",
        "# create a collection of objects from the  id, pages, cleaned_pages, and emb embeddings_array array\n",
        "objects = [{\"id\": id, \"page\": page, \"cleaned_page\": cleaned_page, \"embedding\": embedding.tolist()} for id, page, cleaned_page, embedding in zip(ids, pages, cleaned_pages, embeddings_array)]\n",
        "\n",
        "# Create a JSON-L file with the objects array\n",
        "with open(FILE_NAME, \"w\") as f:\n",
        "    for obj in objects:\n",
        "        f.write(json.dumps(obj) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "ivVsYxVgocib"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ivVsYxVgocib"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write the JSON file to a Cloud Storage Bucket"
      ],
      "metadata": {
        "id": "M89i-yQBqFJM"
      },
      "id": "M89i-yQBqFJM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Cloud Storage bucket named vertex-assessment-dar if it does not already exist\n",
        "from google.cloud import storage\n",
        "\n",
        "BUCKET_NAME = \"vertex-assessment-dar\"\n",
        "BUCKET_URI = \"gs://{0}\".format(BUCKET_NAME)\n",
        "\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "if not bucket.exists():\n",
        "    bucket.create(location=\"us-central1\")\n",
        "\n",
        "# Upload the JSON-L file to the bucket\n",
        "blob = bucket.blob(FILE_NAME)\n",
        "blob.upload_from_filename(FILE_NAME)\n",
        "\n",
        "# print the URI of the bucket\n",
        "print(BUCKET_URI)"
      ],
      "metadata": {
        "id": "KNLEIVbwqJRv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "KNLEIVbwqJRv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a BQ daraset and load the data into it."
      ],
      "metadata": {
        "id": "M-eZ-htn-vzO"
      },
      "id": "M-eZ-htn-vzO"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "bq mk --dataset embeddings_ds\n",
        "\n",
        "bq load --source_format=NEWLINE_DELIMITED_JSON --autodetect embeddings_ds.embeddings_data  gs://vertex-assessment-dar/embeddings_for_bq.jsonl\n"
      ],
      "metadata": {
        "id": "2xyvBf6U-aud"
      },
      "id": "2xyvBf6U-aud",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a BigQuery Query with Python\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT left(cleaned_page, 100) FROM `embeddings_ds.embeddings_data` WHERE ID = 1;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "\n",
        "for row in query_job:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "f26InPA9HCAa"
      },
      "id": "f26InPA9HCAa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}