{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Y0r19Litu1yTVwvcGAGgt1IF"
      },
      "source": [
        "!pip install --upgrade --user google-cloud-aiplatform google-cloud-storage langchain pypdf ratelimit backoff langchain-google-vertexai langchain-openai google-cloud-bigquery"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Y0r19Litu1yTVwvcGAGgt1IF"
    },
    {
      "cell_type": "code",
      "source": [
        "# get project ID\n",
        "PROJECT_ID = ! gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# define project information manually if the above code didn't work\n",
        "if PROJECT_ID == \"(unset)\":\n",
        "  PROJECT_ID = \"[your-project-id]\" # @param {type:\"string\"}\n",
        "\n",
        "print(PROJECT_ID)"
      ],
      "metadata": {
        "id": "it17qlHBurGV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "it17qlHBurGV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Vertex AI"
      ],
      "metadata": {
        "id": "MfUnQ4QIc2aT"
      },
      "id": "MfUnQ4QIc2aT"
    },
    {
      "cell_type": "code",
      "source": [
        "# init the aiplatform package\n",
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "47xjFhdUhYYY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "47xjFhdUhYYY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper funtion to Generate Embeddings"
      ],
      "metadata": {
        "id": "1jujZHnvc60j"
      },
      "id": "1jujZHnvc60j"
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "def text_embedding(question) -> list:\n",
        "    from vertexai.language_models import TextEmbeddingInput\n",
        "    \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
        "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@002\")\n",
        "\n",
        "    question_embedding = TextEmbeddingInput(\n",
        "        text=question,\n",
        "        task_type=\"RETRIEVAL_QUERY\"\n",
        "    )\n",
        "\n",
        "    embeddings = model.get_embeddings([question_embedding])\n",
        "    for embedding in embeddings:\n",
        "        vector = embedding.values\n",
        "    return vector"
      ],
      "metadata": {
        "id": "vtO0EPFDvz81"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vtO0EPFDvz81"
    },
    {
      "cell_type": "code",
      "source": [
        "emb1 = text_embedding(\"Hello World\")\n",
        "print(emb1)"
      ],
      "metadata": {
        "id": "aTkaz6IixJbY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aTkaz6IixJbY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions to answer questions using Gemini and PaLM LLMS\n"
      ],
      "metadata": {
        "id": "qwHRSbATp7RF"
      },
      "id": "qwHRSbATp7RF"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.aiplatform_v1beta1.types.content import SafetySetting, HarmCategory\n",
        "import vertexai\n",
        "from vertexai.preview.generative_models import GenerativeModel, Part\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "def answer_question_gemini(prompt):\n",
        "  model = GenerativeModel(\"gemini-pro\")\n",
        "  response = model.generate_content(\n",
        "    prompt,\n",
        "    generation_config={\n",
        "        \"max_output_tokens\": 8192,\n",
        "        \"temperature\": 0.5,\n",
        "        \"top_p\": 0.5,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "  stream=False,\n",
        "  )\n",
        "  try:\n",
        "    return response.text\n",
        "  except:\n",
        "    print(\"An Error Ocuured Creaning the Data\")\n",
        "    return \"An Error Ocuured Creaning the Data\"\n",
        "\n",
        "\n",
        "def answer_question_palm(prompt):\n",
        "  parameters = {\n",
        "        \"candidate_count\": 1,\n",
        "        \"max_output_tokens\": 1024,\n",
        "        \"temperature\": 0.9,\n",
        "        \"top_p\": 1\n",
        "    }\n",
        "\n",
        "  model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
        "  response = model.predict(\n",
        "        prompt,\n",
        "        **parameters\n",
        "  )\n",
        "  try:\n",
        "    return response.text\n",
        "  except:\n",
        "    print(\"An Error Ocuured Creaning the Data\")\n",
        "    return \"An Error Ocuured Creaning the Data\""
      ],
      "metadata": {
        "id": "ut1JODUio5lb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ut1JODUio5lb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just for fun add helper function to use GPT"
      ],
      "metadata": {
        "id": "wUT8IqPmDX7c"
      },
      "id": "wUT8IqPmDX7c"
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# getpass will prompt for an API Key\n",
        "API_KEY = getpass.getpass(\"Provide your OpenAI API Key\")"
      ],
      "metadata": {
        "id": "fidCO61-pWA_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fidCO61-pWA_"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "def answer_question_gpt(prompt):\n",
        "  llm = OpenAI(openai_api_key=API_KEY)\n",
        "  response = llm.predict(prompt)\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "8s5e3ZI2Dmrq"
      },
      "id": "8s5e3ZI2Dmrq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer_question_gpt(\"Tell me a joke\"))"
      ],
      "metadata": {
        "id": "CwHO1hUoOLiZ"
      },
      "id": "CwHO1hUoOLiZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hepler function to use VECTOR_SEARCH funtion to retrieve data from BQ"
      ],
      "metadata": {
        "id": "MQWawh0qF8if"
      },
      "id": "MQWawh0qF8if"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_closest_pages_from_biguery(question_embedding, num_results=5):\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    client = bigquery.Client()\n",
        "\n",
        "    sql = \"\"\"\n",
        "    SELECT base.cleaned_page, distance\n",
        "    FROM\n",
        "      VECTOR_SEARCH(\n",
        "        TABLE embeddings_ds.embeddings_data,\n",
        "        'embedding',\n",
        "        (SELECT {0} as embedding),\n",
        "        top_k => {1}\n",
        "        );\n",
        "    \"\"\".format(question_embedding, num_results)\n",
        "\n",
        "    query_job = client.query(sql)\n",
        "    data = \"\"\n",
        "    for row in query_job:\n",
        "        data += row.cleaned_page + \"\\n\"\n",
        "        #print(\"Distance: {0} - {1}\".format(row.distance, row.cleaned_page[:50]))\n",
        "    return data"
      ],
      "metadata": {
        "id": "kRkK14q3F6t_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kRkK14q3F6t_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Function to Build the Prompt for the LLM"
      ],
      "metadata": {
        "id": "x1GzVJSpBisC"
      },
      "id": "x1GzVJSpBisC"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(data, question):\n",
        "    prompt = \"\"\"\n",
        "    context: Answer the question using the following Information.\n",
        "\n",
        "    Information: {0}\n",
        "\n",
        "    question: {1}\n",
        "\n",
        "    \"\"\".format(data, question)\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "MR6L3zS-BpUE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MR6L3zS-BpUE"
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "\n",
        "  question_embedding = text_embedding(question)\n",
        "  data = get_closest_pages_from_biguery(question_embedding)\n",
        "  prompt = build_prompt(data, question)\n",
        "\n",
        "  answer_gemini = answer_question_gemini(prompt)\n",
        "  answer_palm = answer_question_palm(prompt)\n",
        "  answer_gpt = answer_question_gpt(prompt)\n",
        "\n",
        "  return answer_gemini, answer_palm, answer_gpt"
      ],
      "metadata": {
        "id": "boaBI8avB5W3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "boaBI8avB5W3"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        "QUESTION = \"What is the minimum safe cooking temperature for chicken?\"\n",
        "QUESTION = \"What is the temperature danger zone?\"\n",
        "QUESTION = \"What are good food handling practices?\"\n",
        "QUESTION = \"What are best practices for handling fish and seafood?\"\n",
        "QUESTION = \"How should you store refrigerated food?\"\n",
        "\n",
        "answer_gemini, answer_palm, answer_gpt = answer_question(QUESTION)\n",
        "\n",
        "\n",
        "print(QUESTION)\n",
        "print(\"--------------------------------\")\n",
        "display(\"Gemini answer: {0}\".format(answer_gemini))\n",
        "print(\"--------------------------------\")\n",
        "display(\"PaLM answer: {0}\".format(answer_palm))\n",
        "print(\"--------------------------------\")\n",
        "display(\"GPT answer: {0}\".format(answer_gpt))\n"
      ],
      "metadata": {
        "id": "qsio4CA35SN9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qsio4CA35SN9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}