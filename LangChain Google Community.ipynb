{
  "cells": [
    {
      "cell_type": "code",
      "id": "Tb0SkhriogTMj1f09IaUHsej",
      "metadata": {
        "tags": [],
        "id": "Tb0SkhriogTMj1f09IaUHsej"
      },
      "source": [
        "%pip install --upgrade langchain-community langchain-core langchain-google-vertexai anthropic[vertex]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "NYC1qEineZyQ"
      },
      "id": "NYC1qEineZyQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Pro Prompt"
      ],
      "metadata": {
        "id": "FF63EIjfDvFY"
      },
      "id": "FF63EIjfDvFY"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import VertexAI\n",
        "\n",
        "# To use model\n",
        "model = VertexAI(model_name=\"gemini-pro\")\n",
        "\n",
        "message = \"Tell me a joke about programming in LISP\"\n",
        "response = model.invoke(message)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "dPqgP2MhA4xz"
      },
      "id": "dPqgP2MhA4xz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming"
      ],
      "metadata": {
        "id": "5fx7maSWCMu0"
      },
      "id": "5fx7maSWCMu0"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "for chunk in model.stream(\"Tell me a scary poem about Java Programming\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "vBKNCH0ACML-"
      },
      "id": "vBKNCH0ACML-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anthropic Claude"
      ],
      "metadata": {
        "id": "cAcU3D6JD4sR"
      },
      "id": "cAcU3D6JD4sR"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    AIMessageChunk,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")\n",
        "from langchain_core.outputs import LLMResult\n",
        "from langchain_google_vertexai.model_garden import ChatAnthropicVertex"
      ],
      "metadata": {
        "id": "VFETDTh2D04e"
      },
      "id": "VFETDTh2D04e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = ! gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# define project information manually if the above code didn't work\n",
        "if PROJECT_ID == \"(unset)\":\n",
        "  PROJECT_ID = \"[your-project-id]\" # @param {type:\"string\"}\n",
        "\n",
        "print(PROJECT_ID)\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Initialise the Model\n",
        "model = ChatAnthropicVertex(\n",
        "    model_name=\"claude-3-sonnet\",\n",
        "    project=project,\n",
        "    location=location,\n",
        ")"
      ],
      "metadata": {
        "id": "UQe0zUNeEWgL"
      },
      "id": "UQe0zUNeEWgL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare input data for the model\n",
        "raw_context = (\n",
        "    \"Your name is Jerry, you are a Standup commedian\"\n",
        "    \"Answer the question, and also tell a joke\"\n",
        ")\n",
        "question = (\n",
        "    \"What is a good movie for kids?\"\n",
        ")\n",
        "context = SystemMessage(content=raw_context)\n",
        "message = HumanMessage(content=question)\n",
        "\n",
        "# Invoke the model\n",
        "response = model.invoke([context, message])\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "I8ORY9LWEdZ6"
      },
      "id": "I8ORY9LWEdZ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Chat App"
      ],
      "metadata": {
        "id": "5bSeAXxKHxWK"
      },
      "id": "5bSeAXxKHxWK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Array to store the message history\n",
        "messages = []\n",
        "\n",
        "# Chatbot context\n",
        "raw_context = (\n",
        "    \"\"\"\n",
        "    Your name is Joey, you are an expert in Python coding and SQL.\n",
        "    You are a very nice and helpful.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Add the context to the Messages array\n",
        "messages.append(SystemMessage(content=raw_context))\n",
        "\n",
        "# Function to get a response (can be replaced with any logic or model)\n",
        "def get_response(user_input):\n",
        "\n",
        "    # Add the users question to the messages array\n",
        "    messages.append(HumanMessage(content=user_input))\n",
        "\n",
        "    # Invoke the model\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # All the model response to the messages array\n",
        "    messages.append(AIMessage(content=response.content))\n",
        "\n",
        "    # Return the response\n",
        "    return response.content\n",
        "\n",
        "# Loop to keep the chat running\n",
        "while True:\n",
        "    # Get input from the user\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check if the user wants to exit the chat\n",
        "    if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "        print(\"ChatBot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Get the response\n",
        "    response = get_response(user_input)\n",
        "\n",
        "    # Display the response\n",
        "    print(f\"ChatBot: {response}\")\n"
      ],
      "metadata": {
        "id": "o2u-0XgrGWcb"
      },
      "id": "o2u-0XgrGWcb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the chat history\n",
        "for message in messages:\n",
        "    print(message)\n",
        "\n",
        "# reset the chat\n",
        "messages = []"
      ],
      "metadata": {
        "id": "jrPGs8w2GlSu"
      },
      "id": "jrPGs8w2GlSu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BigQuery Loader"
      ],
      "metadata": {
        "id": "bXRYjY0FUarD"
      },
      "id": "bXRYjY0FUarD"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-google-community[bigquery]"
      ],
      "metadata": {
        "id": "82GFoJYDUdYG"
      },
      "id": "82GFoJYDUdYG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The query returns an article from Hacker News\n",
        "query = \"\"\"\n",
        "SELECT text\n",
        "FROM `bigquery-public-data.hacker_news.full`\n",
        "where title = \"Another AirBnB Host Horror Story\"\n",
        "limit 1;\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zF3_rQjEUiYE"
      },
      "id": "zF3_rQjEUiYE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_community import BigQueryLoader\n",
        "\n",
        "loader = BigQueryLoader(query)\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "print(data)\n",
        "print(\"-\" * 80)\n",
        "print(data[0].page_content)"
      ],
      "metadata": {
        "id": "kDmw7nClVJII"
      },
      "id": "kDmw7nClVJII",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VertexAI(model_name=\"gemini-pro\")\n",
        "\n",
        "prompt = \"\"\"\n",
        "Summarize the following article in one sentance,plus a few short bullets\n",
        "\n",
        "Article: {0}\n",
        "\n",
        "Summary:\n",
        "\"\"\".format(data[0].page_content)\n",
        "\n",
        "response = model.invoke(prompt)\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "YgNMd5jWWJWv"
      },
      "id": "YgNMd5jWWJWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document AI"
      ],
      "metadata": {
        "id": "WBIA6d41Yn9T"
      },
      "id": "WBIA6d41Yn9T"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-google-community[docai]"
      ],
      "metadata": {
        "id": "m2yhS53JYqop"
      },
      "id": "m2yhS53JYqop",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to create a Cloud Storage Bucket in your project and specify here:\n",
        "GCS_OUTPUT_PATH = \"gs://ocr-processor-output-dar\"\n",
        "\n",
        "# Need to create a Document AI OCR Processor in your project\n",
        "# Create in the Console\n",
        "# Specify the ID here\n",
        "PROCESSOR_NAME = \"projects/982785856251/locations/us/processors/fef610830bfdc9f2\""
      ],
      "metadata": {
        "id": "Ayvs_G8faMsP"
      },
      "id": "Ayvs_G8faMsP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.document_loaders.blob_loaders import Blob\n",
        "from langchain_google_community import DocAIParser\n",
        "\n",
        "parser = DocAIParser(\n",
        "    location=\"us\", processor_name=PROCESSOR_NAME, gcs_output_path=GCS_OUTPUT_PATH\n",
        ")\n",
        "\n",
        "blob = Blob(\n",
        "    path=\"gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/2022Q1_alphabet_earnings_release.pdf\"\n",
        ")\n",
        "\n",
        "docs = list(parser.lazy_parse(blob))\n",
        "print(len(docs))\n"
      ],
      "metadata": {
        "id": "pP4avaoCbGN8"
      },
      "id": "pP4avaoCbGN8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the first 200 characters of each doc\n",
        "for doc in docs:\n",
        "    print(doc.page_content[:200])\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "ujZhHL8sbn1s"
      },
      "id": "ujZhHL8sbn1s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all the pages together\n",
        "DOC = \" \".join([doc.page_content for doc in docs])\n",
        "print(len(DOC))\n"
      ],
      "metadata": {
        "id": "YAd7zS9_doXj"
      },
      "id": "YAd7zS9_doXj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Gemini 1.5 which spports up to 2M token conext (much larger than this example)\n",
        "model = VertexAI(model_name=\"gemini-1.5-pro-001\")\n",
        "\n",
        "prompt = \"\"\"\n",
        "Summarize the following document in a couple sentances,\n",
        "plus a few short bullets that highlight the most important points.\n",
        "\n",
        "Document: {0}\n",
        "\n",
        "Summary:\n",
        "\"\"\".format(DOC)\n",
        "\n",
        "response = model.invoke(prompt)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "TANwVm5Mbiqe"
      },
      "id": "TANwVm5Mbiqe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "LangChain Google Community"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}