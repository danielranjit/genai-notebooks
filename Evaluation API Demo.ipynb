{
  "cells": [
    {
      "cell_type": "code",
      "id": "6Py1fWdO3vokGYiipx0PZVXZ",
      "metadata": {
        "tags": [],
        "id": "6Py1fWdO3vokGYiipx0PZVXZ"
      },
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform==1.55.0 nest-asyncio==1.5.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.preview.evaluation import (\n",
        "    EvalTask,\n",
        "    PromptTemplate,\n",
        "    CustomMetric,\n",
        "    make_metric,\n",
        ")\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import nest_asyncio\n",
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "from vertexai.generative_models import (\n",
        "    GenerativeModel,\n",
        "    HarmCategory,\n",
        "    HarmBlockThreshold\n",
        ")\n",
        "\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "aaEGYFy-V8gw"
      },
      "id": "aaEGYFy-V8gw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = ! gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# define project information manually if the above code didn't work\n",
        "if PROJECT_ID == \"(unset)\":\n",
        "  PROJECT_ID = \"[your-project-id]\" # @param {type:\"string\"}\n",
        "\n",
        "print(PROJECT_ID)\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "3aZFdreiWGy5"
      },
      "id": "3aZFdreiWGy5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Apartment Rental Data"
      ],
      "metadata": {
        "id": "CMtFp8cEKt_j"
      },
      "id": "CMtFp8cEKt_j"
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud storage cp gs://pls-resource-bucket/evaluation-data/apartment_table.csv ."
      ],
      "metadata": {
        "id": "3en8SMK4WTz2"
      },
      "id": "3en8SMK4WTz2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just show what the data looks like.\n",
        "apartment_df = pd.read_csv(\"apartment_table.csv\")\n",
        "apartment_df.head()\n"
      ],
      "metadata": {
        "id": "HKjAM9wRWXMo"
      },
      "id": "HKjAM9wRWXMo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just show 1 record in JSON format\n",
        "apartment_records = apartment_df.to_dict(orient='records')\n",
        "apartment_records[0]"
      ],
      "metadata": {
        "id": "uAzmiPI8WghW"
      },
      "id": "uAzmiPI8WghW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Gemini to create a rental listing based on the features of the apartment"
      ],
      "metadata": {
        "id": "-jVd7DX2LVqw"
      },
      "id": "-jVd7DX2LVqw"
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\n",
        "  \"gemini-1.5-flash-001\",\n",
        "  generation_config={\n",
        "      \"temperature\": 0,\n",
        "      \"top_p\": 0.4,\n",
        "  },\n",
        ")\n",
        "\n",
        "\n",
        "prompt = \"\"\"Write a one-paragraph apartment listing\n",
        "to promote the best features of this apartment: \"\"\"\n",
        "\n",
        "\n",
        "# View the response using Markdown to format it nicely for notebook viewing\n",
        "Markdown(model.generate_content(prompt + str(apartment_records[0])).text)\n"
      ],
      "metadata": {
        "id": "KFKvDCRuWqlg"
      },
      "id": "KFKvDCRuWqlg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dictionary for each record is to a string\n",
        "contexts = [str(record) for record in apartment_records]\n",
        "# Create full prompts by combining the prompt and the context data\n",
        "full_prompts = [prompt + str(record) for record in apartment_records]\n",
        "\n",
        "eval_dataset = pd.DataFrame(\n",
        "  {\n",
        "     # 'content' is used to generate responses\n",
        "      \"content\": full_prompts,\n",
        "     # 'instruction' is considered by some metrics, like fulfillment\n",
        "      \"instruction\": full_prompts,\n",
        "     # 'context' is the information provided to your model to help\n",
        "     # it generate more informed & accurate responses\n",
        "      \"context\": contexts,\n",
        "     # If you had already generated responses for all of your examples\n",
        "     # you could provide them as a list of values with a key of\n",
        "     # 'response' instead of having the evaluation service re-generate\n",
        "     # them.\n",
        "     # \"response\": responses\n",
        "  }\n",
        ")\n"
      ],
      "metadata": {
        "id": "YH_l7N2lYTnt"
      },
      "id": "YH_l7N2lYTnt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for prompt in full_prompts:\n",
        "  print(prompt)"
      ],
      "metadata": {
        "id": "pBt7lyr9ZBO4"
      },
      "id": "pBt7lyr9ZBO4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model's response for each apartment listing using the metric bundles: Fulfillment and Groundedness\n",
        "\n",
        "Fulfillment means the model included everything.\n",
        "\n",
        "Groundedness means the model didn't make anything up.\n"
      ],
      "metadata": {
        "id": "t9yU3_NYLrLK"
      },
      "id": "t9yU3_NYLrLK"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QxzpTpYxMqW0"
      },
      "id": "QxzpTpYxMqW0"
    },
    {
      "cell_type": "code",
      "source": [
        "qa_eval_task = EvalTask(\n",
        "   dataset=eval_dataset,\n",
        "   metrics=[\"fulfillment\", \"groundedness\"],\n",
        "   experiment=\"apartment-listing-generation\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "pDY4UzBGYl3g"
      },
      "id": "pDY4UzBGYl3g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "result = qa_eval_task.evaluate(\n",
        "model=model,\n",
        "      experiment_run_name=f\"apartment-listing-gen-{run_ts}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# You might want to keep track of your results in a list to help\n",
        "# in plotting purposes, as you'll see later on.\n",
        "evaluation_results = []\n",
        "evaluation_results.append(result)\n"
      ],
      "metadata": {
        "id": "GtVxoQxRYwMX"
      },
      "id": "GtVxoQxRYwMX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_eval_report(eval_result, metrics=None):\n",
        "   \"\"\"Display the evaluation results.\"\"\"\n",
        "\n",
        "\n",
        "   title, summary_metrics, report_df = eval_result\n",
        "   metrics_df = pd.DataFrame.from_dict(summary_metrics, orient=\"index\").T\n",
        "   if metrics:\n",
        "       metrics_df = metrics_df.filter(\n",
        "           [\n",
        "               metric\n",
        "               for metric in metrics_df.columns\n",
        "               if any(selected_metric in metric for selected_metric in metrics)\n",
        "           ]\n",
        "       )\n",
        "       report_df = report_df.filter(\n",
        "           [\n",
        "               metric\n",
        "               for metric in report_df.columns\n",
        "               if any(selected_metric in metric for selected_metric in metrics)\n",
        "           ]\n",
        "       )\n",
        "\n",
        "\n",
        "   # Display the title with Markdown for emphasis\n",
        "   display(Markdown(f\"## {title}\"))\n",
        "\n",
        "\n",
        "   # Display the metrics DataFrame\n",
        "   display(Markdown(\"### Summary Metrics\"))\n",
        "   display(metrics_df)\n",
        "\n",
        "\n",
        "   # Display the detailed report DataFrame\n",
        "   display(Markdown(f\"### Report Metrics\"))\n",
        "   display(report_df)\n",
        "\n",
        "\n",
        "display_eval_report(((\"Eval Result\", result.summary_metrics, result.metrics_table)))\n"
      ],
      "metadata": {
        "id": "g4ub0yWrZ6cX"
      },
      "id": "g4ub0yWrZ6cX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.metrics_table[\"groundedness/explanation\"][7])"
      ],
      "metadata": {
        "id": "6A0ao5Ivaf-5"
      },
      "id": "6A0ao5Ivaf-5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a minor update, but should stop the model from inventing\n",
        "# as many details about each apartment.\n",
        "updated_prompt = \"Write an apartment listing promoting the best features of this apartment. Use only the details included in the following information: \"\n",
        "\n",
        "\n",
        "updated_full_prompts = [updated_prompt + str(record) for record in apartment_records]\n",
        "\n",
        "\n",
        "eval_dataset = pd.DataFrame(\n",
        "   {\n",
        "       \"content\": updated_full_prompts,\n",
        "       \"instruction\": updated_full_prompts,\n",
        "       \"context\": contexts, # The contexts haven't changed\n",
        "   }\n",
        ")\n"
      ],
      "metadata": {
        "id": "jmNLeDJmcIRg"
      },
      "id": "jmNLeDJmcIRg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_eval_task = EvalTask(\n",
        "   dataset=eval_dataset,\n",
        "   metrics=[\"fulfillment\", \"groundedness\"],\n",
        "   experiment=\"apartment-listing-generation\",\n",
        ")\n",
        "\n",
        "\n",
        "run_ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "result = qa_eval_task.evaluate(\n",
        "\tmodel=model,\n",
        "      experiment_run_name=f\"apartment-listing-gen-{run_ts}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Append the new result to your results\n",
        "evaluation_results.append(result)\n",
        "\n",
        "\n",
        "# Preview the results\n",
        "display_eval_report(((\"Eval Result 2\", result.summary_metrics, result.metrics_table)))\n"
      ],
      "metadata": {
        "id": "m9gnEtfIcONm"
      },
      "id": "m9gnEtfIcONm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluation_results[0])"
      ],
      "metadata": {
        "id": "4VM4DebpdwSw"
      },
      "id": "4VM4DebpdwSw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "def plot_bar_plot(eval_results, metrics=None):\n",
        "   fig = go.Figure()\n",
        "   data = []\n",
        "\n",
        "\n",
        "   for eval_result in eval_results:\n",
        "       #title, summary_metrics, _ = eval_result\n",
        "       #title = eval_result.title\n",
        "       summary_metrics = eval_result.summary_metrics\n",
        "       if metrics:\n",
        "           summary_metrics = {\n",
        "               k: summary_metrics[k]\n",
        "               for k, v in summary_metrics.items()\n",
        "               if any(selected_metric in k for selected_metric in metrics)\n",
        "           }\n",
        "\n",
        "\n",
        "       data.append(\n",
        "           go.Bar(\n",
        "               x=list(summary_metrics.keys()),\n",
        "               y=list(summary_metrics.values()),\n",
        "               #name=title,\n",
        "           )\n",
        "       )\n",
        "\n",
        "\n",
        "   fig = go.Figure(data=data)\n",
        "\n",
        "\n",
        "   # Change the bar mode\n",
        "   fig.update_layout(barmode=\"group\")\n",
        "   fig.show()\n",
        "\n",
        "\n",
        "plot_bar_plot(evaluation_results, metrics=[\"fulfillment/mean\", \"groundedness/mean\"])\n"
      ],
      "metadata": {
        "id": "BcS3o3eJcequ"
      },
      "id": "BcS3o3eJcequ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluation_results[0])"
      ],
      "metadata": {
        "id": "TPiZReZWdRb6"
      },
      "id": "TPiZReZWdRb6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Evaluation API Demo"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}